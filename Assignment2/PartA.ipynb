{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part A (Basic Neural Network Structuring)</h1>\n",
    "\n",
    "<p>First actual neural networks let's go. First, for Part A, we'll just be perfecting the ability of a basic neural network to do a forward pass. The most important thing to determine for this task is the shapes that all of our data should have. Let's stat with some basic inputs then we can describe this in comments as we go.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports (Nice and minimal)\n",
    "import math\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Class Definition</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    \n",
    "    # When we initialize a neural network the only information that we give it \n",
    "    # will be a list of the size of each layer in the network.\n",
    "    def __init__(self, layer_sizes):\n",
    "        \n",
    "        # Get the number of layers that the user wants the network to contain.\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.num_layers = len(self.layer_sizes)\n",
    "        \n",
    "        # Build the weights for our network.\n",
    "        # The layout for the weights will be a list of matrices for now.\n",
    "        # The shape of a given weight item in the list will be [from layer size + 1, to layer size]\n",
    "        # We add the plus 1 in the from size to account for the bias neuron of that layer.\n",
    "        self.thetas = []\n",
    "        for layer_ix in range(1, len(layer_sizes)):\n",
    "            self.thetas.append((1/(np.sqrt(layer_sizes[layer_ix-1]+1))) * torch.rand(layer_sizes[layer_ix-1]+1, layer_sizes[layer_ix]))\n",
    "            \n",
    "    # For the NeuralNetwork class getLayer() just sends the caller the list of weights\n",
    "    # for the entire network.\n",
    "    def getLayer(self, layer):\n",
    "        return self.thetas[layer]\n",
    "        \n",
    "    # Where the magic happens for this class. When the user calls the forward method \n",
    "    # for a NeuralNetwork object with an input list of integers, we feed those\n",
    "    # integers into the network, multiplying by each weight matrix.\n",
    "    def forward(self, input_list):\n",
    "\n",
    "        # Initialize a list of all of the weighted sums that we will get from \n",
    "        # multiplying a previous layer by the connection weights.\n",
    "        z = []\n",
    "        \n",
    "        # Initialize a list of all of the sigmoid nonlinearity results\n",
    "        # This will end up being the same size as the weighted sums (z)\n",
    "        a = []\n",
    "        \n",
    "        # This is just a PyTorch tensor created from the users' input and \n",
    "        # reshaped to be a horizontal row of inputs.\n",
    "        first_layer_no_bias = torch.FloatTensor(input_list).view(1, len(input_list))\n",
    "        \n",
    "        # Add our first z values which will be the input layer. Since there is no nonlinearity\n",
    "        # applied to the input before sending it into the network, we just set the first\n",
    "        # a value to be the input array as well.\n",
    "        z.append(first_layer_no_bias)\n",
    "        a.append(z[0])\n",
    "                    \n",
    "        # Iterate through each layer of the matrix until 1 before the output layer.\n",
    "        for i in range(0,self.num_layers-1):\n",
    "            \n",
    "            # Create a PyTorch tensor with a single value and then concatenate it \n",
    "            # onto the front of the input array. This is our bias value and when \n",
    "            # concatenated it creates our input along with the bias. \n",
    "            # We add a bias item to every layer within this loop\n",
    "            bias = torch.ones(1, 1)\n",
    "            layer_with_bias = torch.cat([bias, a[i]], 1)\n",
    "          \n",
    "            # Print statements to help figure out whether our theta and a/z sizes are correct.\n",
    "            # Matrices can only be multiplied if they share the same inner dimension and their\n",
    "            # result of mulitplication should be their outer dimension.\n",
    "            # Eg. [8x3] matrix multiplied with [3x4] matrix will result in an [8x4] matrix.\n",
    "            print(\"Layer %d 'in' size: %dx%d\" % (i, layer_with_bias.shape[0], layer_with_bias.shape[1]))        \n",
    "            print(\"Layer %d theta size: %dx%d\" % (i, self.thetas[i].shape[0], self.thetas[i].shape[1]))\n",
    "            \n",
    "            # Create the input to our sigmoid nonlinearity by matrix multiplying \n",
    "            # the current layer that we're on with the appropriate weight matrix.\n",
    "            z.append(torch.mm(layer_with_bias, self.thetas[i]))\n",
    "            a.append(1/(1+np.exp(-z[-1])))\n",
    "            \n",
    "            print(\"Layer %d result size: %dx%d\" % (i, a[-1].shape[0], a[-1].shape[1]))\n",
    "\n",
    "        # This is a bit of a bug, I'm getting outputs that are not in fact shaped according \n",
    "        # to any laws of linear algebra. For now just taking the first element of the very last\n",
    "        # a value has been doing just fine. \n",
    "        result = a[-1][0][0]\n",
    "        \n",
    "        # If binary output desired:\n",
    "        if result >= 0.5:\n",
    "            result = 1.0\n",
    "        elif result < 0.5:\n",
    "            result = 0.0\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Layer Size Definition and Network Instantiation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [2, 2, 1, 5, 2, 3]\n",
    "network_model = NeuralNetwork(layer_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>getLayer() Method Testing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[0.0759, 0.3229],\n",
      "        [0.5397, 0.2314],\n",
      "        [0.2726, 0.4945]]), tensor([[0.4681],\n",
      "        [0.5301],\n",
      "        [0.4850]]), tensor([[0.6772, 0.7051, 0.0722, 0.3891, 0.7057],\n",
      "        [0.6743, 0.1852, 0.2623, 0.1204, 0.6807]]), tensor([[0.3946, 0.3366],\n",
      "        [0.2986, 0.0963],\n",
      "        [0.0875, 0.2353],\n",
      "        [0.1303, 0.1170],\n",
      "        [0.0096, 0.0210],\n",
      "        [0.1787, 0.0819]])]\n"
     ]
    }
   ],
   "source": [
    "thetas = [network_model.getLayer(x) for x in range(len(layer_sizes)-2)]\n",
    "print(thetas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Feeding the Neural Network/Forward Pass</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 'in' size: 1x3\n",
      "Layer 0 theta size: 3x2\n",
      "Layer 0 result size: 1x2\n",
      "Layer 1 'in' size: 1x3\n",
      "Layer 1 theta size: 3x1\n",
      "Layer 1 result size: 1x1\n",
      "Layer 2 'in' size: 1x2\n",
      "Layer 2 theta size: 2x5\n",
      "Layer 2 result size: 1x5\n",
      "Layer 3 'in' size: 1x6\n",
      "Layer 3 theta size: 6x2\n",
      "Layer 3 result size: 1x2\n",
      "Layer 4 'in' size: 1x3\n",
      "Layer 4 theta size: 3x3\n",
      "Layer 4 result size: 1x3\n",
      "Random test of network: \n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "input_array = [42, 42]\n",
    "network_result = network_model.forward(input_array)\n",
    "print(\"Random test of network: \")\n",
    "print(network_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "deep_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
