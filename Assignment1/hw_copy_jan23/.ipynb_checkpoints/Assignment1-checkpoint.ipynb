{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Assignment 1 Notebook</h1>\n",
    "\n",
    "<p>This notebook is meant to serve as a sort of in-depth README file for this assignment (because I haven't bothered to make a README yet and I like Jupyter Notebooks a bit better personally. I'll go through everything I do in our class to create the convolution object that we will be messing with for this assignment as well as create some basic templates for images and other important functions for deep learning with PyTorch.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Imports</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import PIL\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Image Loading</h3>\n",
    "\n",
    "<p>We're gonna create a function that just takes an image path and then returns both a black and white and a colored version of the image after displaying the image.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(filepath):\n",
    "    original_image = PIL.Image.open(filepath)\n",
    "    black_and_white = original_image.convert('L')\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    fig.add_subplot(121)\n",
    "    plt.imshow(original_image)\n",
    "    plt.title('Original Loaded Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    fig.add_subplot(122)\n",
    "    plt.imshow(black_and_white)\n",
    "    plt.title('Black and White Image')\n",
    "    plt.axis('off')\n",
    "   \n",
    "    plt.show()\n",
    "    \n",
    "    return [original_image, black_and_white]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Class Creation</h3>\n",
    "\n",
    "<p>This is where we'll create the actual Conv2D class so that other files can call the convolution methods that we're defining here. That doesn't work exactly so in a Jupyter Notebook, but it's ok because we've got a main.py script calling a conv.py script class object in this same directory so you can check in that file if you'd like to see how to invoke the Conv2D object outside of a given script with an import statement.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-50-298df48a1ab6>, line 37)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-50-298df48a1ab6>\"\u001b[0;36m, line \u001b[0;32m37\u001b[0m\n\u001b[0;31m    [-, -1, 0]\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class Conv2D(object):\n",
    "\n",
    "    def __init__(self, in_channel, o_channel, stride, mode):\n",
    "        self.in_channel = in_channel\n",
    "        self.o_channel = o_channel\n",
    "        self.stride = stride\n",
    "        self.mode = mode\n",
    "\n",
    "    def forward(self, input_image):\n",
    "\n",
    "        # Set up kernels\n",
    "        kernels = []\n",
    "\n",
    "        horizontal_edge_kernel = torch.tensor([\n",
    "            [-1, -1, -1], \n",
    "            [ 0, 0, 0], \n",
    "            [1, 1, 1]\n",
    "        ])\n",
    "        vertical_edge_kernel = torch.tensor([\n",
    "            [-1, 0, 1], \n",
    "            [-1, 0, 1], \n",
    "            [-1, 0, 1]\n",
    "        ])\n",
    "        whiten_kernel = torch.tensor([\n",
    "            [1, 1, 1], \n",
    "            [1, 1, 1], \n",
    "            [1, 1, 1]\n",
    "        ])\n",
    "        edge_detect_kernel = torch.tensor([\n",
    "             [-1, -1, -1],\n",
    "             [-1,  8, -1],  \n",
    "             [-1, -1, -1]\n",
    "        ])\n",
    "        sharpen_kernel = torch.tensor([\n",
    "            [0, -1, 0],\n",
    "            [-1, 5, -1],\n",
    "            [-, -1, 0]\n",
    "        ])\n",
    "       \n",
    "        num_kernels = self.o_channel\n",
    "\n",
    "        if self.mode == 'rand':\n",
    "            kernel_size = 5\n",
    "            for i in range(0,num_kernels):\n",
    "                kernels.append(torch.randint(-10, 10, (kernel_size, kernel_size)))\n",
    "        elif self.mode == 'horizontal_edges':\n",
    "            kernel_size = 3\n",
    "            for i in range(0,num_kernels):\n",
    "                kernels.append(horizontal_edge_kernel)\n",
    "        elif self.mode == 'vertical_edges':\n",
    "            kernel_size = 3\n",
    "            for i in range(0,num_kernels):\n",
    "                kernels.append(vertical_edge_kernel)\n",
    "        elif self.mode == 'edges':\n",
    "            kernel_size = 3\n",
    "            for i in range(0,num_kernels):\n",
    "                kernels.append(edge_detect_kernel)\n",
    "        elif self.mode == 'whiten':\n",
    "            kernel_size = 3\n",
    "            for i in range(0,num_kernels):\n",
    "                kernels.append(whiten_kernel)\n",
    "        elif self.mode == 'sharpen':\n",
    "            kernel_size = 3\n",
    "            for i in range(0,num_kernels):\n",
    "                kernels.append(sharpen_kernel)\n",
    "\n",
    "        # Convert image object to tensor\n",
    "        image_array = np.array(input_image)\n",
    "        image_tensor = torch.from_numpy(image_array)\n",
    "        dimensions_list = list(image_tensor.shape)\n",
    "       \n",
    "        padding = int((kernel_size) / 2)\n",
    "\n",
    "        if self.in_channel > 1:\n",
    "            padded_tensor = torch.zeros(dimensions_list[0] + padding, dimensions_list[1] + padding, dimensions_list[2])\n",
    "            padded_tensor[1:dimensions_list[0]+1, 1:dimensions_list[1]+1, :] = image_tensor\n",
    "        else:\n",
    "            padded_tensor = torch.zeros(dimensions_list[0] + padding, dimensions_list[1] + padding)\n",
    "            padded_tensor[1:dimensions_list[0]+1, 1:dimensions_list[1]+1] = image_tensor\n",
    "\n",
    "        num_rows = dimensions_list[0]\n",
    "        num_cols = dimensions_list[1]\n",
    "\n",
    "        output_rows = int((num_rows - kernel_size + 2*padding) / self.stride + 1)\n",
    "        output_cols = int((num_cols - kernel_size + 2*padding) / self.stride + 1)\n",
    "\n",
    "        print(\"Input image resolution: %dx%d.\\n\" % (num_rows, num_cols), end=\"\", flush=True)\n",
    "\n",
    "        # Setting up the output array\n",
    "        output_tensors = [torch.zeros(output_rows, output_cols) for x in kernels]\n",
    "        num_operations = [0 for x in kernels]\n",
    "\n",
    "        for i in range(0,num_kernels):\n",
    "            print(\"\\nKernel: \")\n",
    "            print(kernels[i])\n",
    "            print(\"\\nCurrent kernel number: %d\" % (i+1))\n",
    "            for channel in range(0,self.in_channel):\n",
    "                print(\"Current input channel: %d\" % (channel+1))\n",
    "                half_kernel = math.floor(kernel_size / 2)\n",
    "                # Iterate through each row of the image (outer loop -- y)\n",
    "                row_out = 0\n",
    "                for row in range(half_kernel, num_rows-half_kernel, self.stride):\n",
    "                    # Iterate through each column of the image (inner loop -- x)\n",
    "                    col_out = 0\n",
    "                    for col in range(half_kernel, num_cols-half_kernel, self.stride): \n",
    "                        num_operations[i] = num_operations[i] + kernel_size + kernel_size-1\n",
    "\n",
    "                        if self.in_channel > 1:\n",
    "                            region_of_interest = padded_tensor[row-half_kernel:row+half_kernel+1, col-half_kernel:col+half_kernel+1, channel]    \n",
    "                        else:\n",
    "                            region_of_interest = padded_tensor[row-half_kernel:row+half_kernel+1, col-half_kernel:col+half_kernel+1]    \n",
    "                        \n",
    "                        region_of_interest = region_of_interest.double()\n",
    "                        kernel = kernels[i].double()\n",
    "\n",
    "                        output_tensors[i][row_out, col_out] = output_tensors[i][row_out, col_out] + torch.tensordot(region_of_interest, kernel)\n",
    "                        \n",
    "                        col_out = col_out + 1\n",
    "                    row_out = row_out + 1\n",
    "\n",
    "        output_tensors = [torch.clamp(output_tensor, min=0, max=255) for output_tensor in output_tensors]\n",
    "\n",
    "        return [num_operations, output_tensors]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Class Invocation</h3>\n",
    "\n",
    "<p>Here we'll get an image with our previously defined function and actually instantiate an object for the convolution class. To perform a convolution on it. Since the assignment main and conv files already show how to do each teask in the assignment I'd like to extend the functionality of the class a bit and make it so that a user can supply a list of possible image convolution methods and the class choose an appropriate kernel for that task. That means o_channel should always be 1 when invoking this class in this notebook.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensors_to_images(tensors, conv2d, output_name):\n",
    "\n",
    "    conv_result_images = []\n",
    "\n",
    "    # Take the returned tensor convolution result and turn it into an image.\n",
    "    # A user-defined function would be super useful for this but the instructions\n",
    "    # don't mention any functions other than main being allowed for this file\n",
    "    for i, tensor in enumerate(tensors):\n",
    "        # Normalize the convolution output tensor to a 0-1 scale\n",
    "        tensor = (tensor - torch.min(tensor)) / (torch.max(tensor) - torch.min(tensor))\n",
    "\n",
    "        # Convert the tensor to a numpy array and then normalize to 0-255 scale\n",
    "        numpy_result = tensor.numpy()\n",
    "        numpy_result = numpy_result * 255\n",
    "        \n",
    "        # Convert the numpy array to a PIL image and then convert it to black and white\n",
    "        conv_result_images.append(PIL.Image.fromarray(numpy_result))\n",
    "        conv_result_images.append(conv_result_images[-1].convert('RGB'))\n",
    "\n",
    "        # Save the resultant image as a png image file\n",
    "        conv_result_images[-1].save('results/'+output_name+'.png', 'PNG')\n",
    "        print('Convolution image saved as results/'+output_name+'.png')\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.imshow(conv_result_images[-1])\n",
    "        plt.axis('off')\n",
    "        plt.title('Resultant Image')\n",
    "    \n",
    "    return conv_result_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provided_path = 'images/checkerboard.png'\n",
    "[checkerboard, checkerboard_bw] = get_image(provided_path)\n",
    "output_name_provided = 'checkerboard_horizontal'\n",
    "\n",
    "personal_path = 'images/HTTT.jpg'\n",
    "[thief, thief_bw] = get_image(personal_path)\n",
    "output_name_personal = 'thief_edges'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel options: horizontal_edges, vertical_edges, edges, whiten, sharpen\n",
    "\n",
    "conv2d_bw_input = Conv2D(\n",
    "    in_channel=1,\n",
    "    o_channel=1,\n",
    "    stride=1,\n",
    "    mode='horizontal_edges'\n",
    ")\n",
    "\n",
    "conv2d_color_input = Conv2D(\n",
    "    in_channel=3,\n",
    "    o_channel=1,\n",
    "    stride=1,\n",
    "    mode='sharpen'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[num_operations, output_tensors] = conv2d_bw_input.forward(checkerboard_bw)\n",
    "tensors_to_images(output_tensors, conv2d_bw_input, output_name_provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[num_operations, output_tensors] = conv2d_color_input.forward(thief)\n",
    "tensors_to_images(output_tensors, conv2d_color_input, output_name_personal"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "deep_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
