{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Assignment 1 Notebook</h1>\n",
    "\n",
    "<p>This is a Jupyter Notebook created to accompany the first assignment submission for BME495 Deep learning. This notebook is meant to be a bit more functional than the actual assignment and give a bit more flexibility so that I can do some fun things like verbally choose images I want and convolution kernels that I want for them. So with that let's hop in.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Imports</h3>\n",
    "\n",
    "<p>The main important imports here (at least for my fledgling deep learning knowledge) is going to be torch and time to see how long torch things take to happen.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "import PIL\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Image Loading</h3>\n",
    "\n",
    "<p>We're going to create a function that just takes an image path and then returns both a black and white and a colored version of the image after displaying the image. This will make it way easier for us to import images later. My thinking for now is going to be to create a list of filepaths that I can change or add to and then iterate through those, running this function to create a set of images that I want convolved.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(filepath):\n",
    "    original_image = PIL.Image.open(filepath)\n",
    "    black_and_white = original_image.convert('L')\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    fig.add_subplot(121)\n",
    "    plt.imshow(original_image)\n",
    "    plt.title('Original Loaded Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    fig.add_subplot(122)\n",
    "    plt.imshow(black_and_white)\n",
    "    plt.title('Black and White Image')\n",
    "    plt.axis('off')\n",
    "   \n",
    "    plt.show()\n",
    "    \n",
    "    return [original_image, black_and_white]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Class Creation</h3>\n",
    "\n",
    "<p>Since we're creating this class within a Jupyter Notebook we lose some of the cross-script functionality that we get by making a conv.py script, however we can accomplish the same goals and instantiate this class throughout our notebook just like we would if we were to import the class definitions from another script.</p>\n",
    "\n",
    "<p>The main differences between this notebook's Conv2D class and the one defined in conv.py is a difference in inputs. This notebook only takes in_channel, o_channel, stride, and mode, compared to conv.py which also specifies kernel_size. The reason for this was so that I could make a bit of an easier process performing a convolution in favor of a bit of an easier user experience. I've also removed the task argument to the class because it's very specific to this homework assignment for BME495, and in other instances of convolving an image it would be useless.</p>\n",
    "\n",
    "<p>When you invoke an instance of this class now, the in_channel, o_channel, and stride arguments remain the same, however mode now takes a string argument to verbally describe what the kernel should do. We have a list of kernels defined in the class such as horizontal_edge_kernel and others that can be invoked with prompted types of convolutions below.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(object):\n",
    "\n",
    "    def __init__(self, in_channel, o_channel, stride, mode):\n",
    "        self.in_channel = in_channel\n",
    "        self.o_channel = o_channel\n",
    "        self.stride = stride\n",
    "        self.mode = mode\n",
    "\n",
    "    def forward(self, input_image):\n",
    "\n",
    "        # Set up kernels\n",
    "        kernels = []\n",
    "\n",
    "        horizontal_edge_kernel = torch.tensor([\n",
    "            [-1, -1, -1], \n",
    "            [ 0, 0, 0], \n",
    "            [1, 1, 1]\n",
    "        ])\n",
    "        vertical_edge_kernel = torch.tensor([\n",
    "            [-1, 0, 1], \n",
    "            [-1, 0, 1], \n",
    "            [-1, 0, 1]\n",
    "        ])\n",
    "        whiten_kernel = torch.tensor([\n",
    "            [1, 1, 1], \n",
    "            [1, 1, 1], \n",
    "            [1, 1, 1]\n",
    "        ])\n",
    "        edge_detect_kernel = torch.tensor([\n",
    "             [-1, -1, -1],\n",
    "             [-1,  8, -1],  \n",
    "             [-1, -1, -1]\n",
    "        ])\n",
    "        sharpen_kernel = torch.tensor([\n",
    "            [0, -1, 0],\n",
    "            [-1, 5, -1],\n",
    "            [0, -1, 0]\n",
    "        ])\n",
    "       \n",
    "        num_kernels = self.o_channel\n",
    "\n",
    "        if self.mode == 'rand':\n",
    "            kernel_size = 5\n",
    "            for i in range(0,num_kernels):\n",
    "                kernels.append(torch.randint(-10, 10, (kernel_size, kernel_size)))\n",
    "        elif self.mode == 'horizontal_edges':\n",
    "            kernel_size = 3\n",
    "            for i in range(0,num_kernels):\n",
    "                kernels.append(horizontal_edge_kernel)\n",
    "        elif self.mode == 'vertical_edges':\n",
    "            kernel_size = 3\n",
    "            for i in range(0,num_kernels):\n",
    "                kernels.append(vertical_edge_kernel)\n",
    "        elif self.mode == 'edges':\n",
    "            kernel_size = 3\n",
    "            for i in range(0,num_kernels):\n",
    "                kernels.append(edge_detect_kernel)\n",
    "        elif self.mode == 'whiten':\n",
    "            kernel_size = 3\n",
    "            for i in range(0,num_kernels):\n",
    "                kernels.append(whiten_kernel)\n",
    "        elif self.mode == 'sharpen':\n",
    "            kernel_size = 3\n",
    "            for i in range(0,num_kernels):\n",
    "                kernels.append(sharpen_kernel)\n",
    "\n",
    "        # Convert image object to tensor\n",
    "        image_array = np.array(input_image)\n",
    "        image_tensor = torch.from_numpy(image_array)\n",
    "        dimensions_list = list(image_tensor.shape)\n",
    "       \n",
    "        padding = int((kernel_size) / 2)\n",
    "\n",
    "        if self.in_channel > 1:\n",
    "            padded_tensor = torch.zeros(dimensions_list[0] + padding, dimensions_list[1] + padding, dimensions_list[2])\n",
    "            padded_tensor[1:dimensions_list[0]+1, 1:dimensions_list[1]+1, :] = image_tensor\n",
    "        else:\n",
    "            padded_tensor = torch.zeros(dimensions_list[0] + padding, dimensions_list[1] + padding)\n",
    "            padded_tensor[1:dimensions_list[0]+1, 1:dimensions_list[1]+1] = image_tensor\n",
    "\n",
    "        num_rows = dimensions_list[0]\n",
    "        num_cols = dimensions_list[1]\n",
    "\n",
    "        output_rows = int((num_rows - kernel_size + 2*padding) / self.stride + 1)\n",
    "        output_cols = int((num_cols - kernel_size + 2*padding) / self.stride + 1)\n",
    "\n",
    "        print(\"Input image resolution: %dx%d.\\n\" % (num_rows, num_cols), end=\"\", flush=True)\n",
    "\n",
    "        # Setting up the output array\n",
    "        output_tensors = [torch.zeros(output_rows, output_cols) for x in kernels]\n",
    "        num_operations = [0 for x in kernels]\n",
    "\n",
    "        for i in range(0,num_kernels):\n",
    "            print(\"\\nKernel: \")\n",
    "            print(kernels[i])\n",
    "            print(\"\\nCurrent kernel number: %d\" % (i+1))\n",
    "            for channel in range(0,self.in_channel):\n",
    "                print(\"Current input channel: %d\" % (channel+1))\n",
    "                half_kernel = math.floor(kernel_size / 2)\n",
    "                # Iterate through each row of the image (outer loop -- y)\n",
    "                row_out = 0\n",
    "                for row in range(half_kernel, num_rows-half_kernel, self.stride):\n",
    "                    # Iterate through each column of the image (inner loop -- x)\n",
    "                    col_out = 0\n",
    "                    for col in range(half_kernel, num_cols-half_kernel, self.stride): \n",
    "                        num_operations[i] = num_operations[i] + kernel_size + kernel_size-1\n",
    "\n",
    "                        if self.in_channel > 1:\n",
    "                            region_of_interest = padded_tensor[row-half_kernel:row+half_kernel+1, col-half_kernel:col+half_kernel+1, channel]    \n",
    "                        else:\n",
    "                            region_of_interest = padded_tensor[row-half_kernel:row+half_kernel+1, col-half_kernel:col+half_kernel+1]    \n",
    "                        \n",
    "                        region_of_interest = region_of_interest.double()\n",
    "                        kernel = kernels[i].double()\n",
    "\n",
    "                        output_tensors[i][row_out, col_out] = output_tensors[i][row_out, col_out] + torch.tensordot(region_of_interest, kernel)\n",
    "                        \n",
    "                        col_out = col_out + 1\n",
    "                    row_out = row_out + 1\n",
    "\n",
    "        output_tensors = [torch.clamp(output_tensor, min=0, max=255) for output_tensor in output_tensors]\n",
    "\n",
    "        return [num_operations, output_tensors]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tensor Converter Function</h3>\n",
    "\n",
    "<p>You'll notice that the output of the forward() method for our Conv2D class above is a list of tensors along with number of operations that were performed by the forward() method. For a given input there might be several output images depending on how many kernels you want to use, so we make it output_tensors instead of just output_tensor. Since this tensor doesn't do us a lot of phsyical good we have to actually convert it back to an image and save it to our machine. The function below makes this a lot quicker of a process than manually converting every time.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensors_to_images(tensors, conv2d, output_name):\n",
    "\n",
    "    conv_result_images = []\n",
    "\n",
    "    # Take the returned tensor convolution result and turn it into an image.\n",
    "    # A user-defined function would be super useful for this but the instructions\n",
    "    # don't mention any functions other than main being allowed for this file\n",
    "    for i, tensor in enumerate(tensors):\n",
    "        # Normalize the convolution output tensor to a 0-1 scale\n",
    "        tensor = (tensor - torch.min(tensor)) / (torch.max(tensor) - torch.min(tensor))\n",
    "\n",
    "        # Convert the tensor to a numpy array and then normalize to 0-255 scale\n",
    "        numpy_result = tensor.numpy()\n",
    "        numpy_result = numpy_result * 255\n",
    "        \n",
    "        # Convert the numpy array to a PIL image and then convert it to black and white\n",
    "        conv_result_images.append(PIL.Image.fromarray(numpy_result))\n",
    "        conv_result_images.append(conv_result_images[-1].convert('RGB'))\n",
    "\n",
    "        # Save the resultant image as a png image file\n",
    "        conv_result_images[-1].save('results/'+output_name+'.png', 'PNG')\n",
    "        print('Convolution image saved as results/'+output_name+'.png')\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.imshow(conv_result_images[-1])\n",
    "        plt.axis('off')\n",
    "        plt.title('Resultant Image')\n",
    "    \n",
    "    return conv_result_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Image Loading</h3>\n",
    "\n",
    "<p>Let's grab some images from our machine that we're going to convolve.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provided_path = 'images/checkerboard.png'\n",
    "[checkerboard, checkerboard_bw] = get_image(provided_path)\n",
    "output_name_provided = 'checkerboard_horizontal'\n",
    "\n",
    "personal_path = 'images/HTTT.jpg'\n",
    "[thief, thief_bw] = get_image(personal_path)\n",
    "output_name_personal = 'thief_edges'\n",
    "\n",
    "ben_path = 'images/ben.jpeg'\n",
    "[ben, ben_bw] = get_image(ben_path)\n",
    "output_name_ben = 'ben_edges'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Conv2D Instantation</h3>\n",
    "\n",
    "<p>Let's provide the class with all of the arguments we want for a given image and give specific names to each. I create two here with the main difference being the ability to handle a black and white or a colored image, but creating more that would have different kernels to convolve the image with would work just as well here.</p>\n",
    "\n",
    "<h4>Possible Mode Arguments:</h4>\n",
    "<ul>\n",
    "    <li>horizontal_edges</li>\n",
    "    <li>vertical_edges</li>\n",
    "    <li>edges</li>\n",
    "    <li>sharpen</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d_bw_input = Conv2D(\n",
    "    in_channel=1,\n",
    "    o_channel=1,\n",
    "    stride=1,\n",
    "    mode='horizontal_edges'\n",
    ")\n",
    "\n",
    "conv2d_color_input = Conv2D(\n",
    "    in_channel=3,\n",
    "    o_channel=1,\n",
    "    stride=1,\n",
    "    mode='sharpen'\n",
    ")\n",
    "\n",
    "conv2d_edges = Conv2D(\n",
    "    in_channel=3,\n",
    "    o_channel=1,\n",
    "    stride=1,\n",
    "    mode='edges'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Convolution Time</h3>\n",
    "\n",
    "<p>What we've all been waiting for, let's enact that forward method on both classes!</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[num_operations, output_tensors] = conv2d_bw_input.forward(checkerboard_bw)\n",
    "tensors_to_images(output_tensors, conv2d_bw_input, output_name_provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[num_operations, output_tensors] = conv2d_color_input.forward(thief)\n",
    "tensors_to_images(output_tensors, conv2d_color_input, output_name_personal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[num_operations, output_tensors] = conv2d_edges.forward(ben)\n",
    "tensors_to_images(output_tensors, conv2d_color_input, output_name_ben)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "deep_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
